## 一、写在前面

机器学习的问题大致可以分为分类问题和回归问题。分类问题是数据属于哪一个类别的问题。比如，区分图像中的人是男性还是女性的问题就是分类问题。而回归问题是根据某个输入预测一个（连续的）数值的问题。比如，根据一个人的图像预测这个人的体重的问题就
是回归问题（类似“57.4kg”这样的预测）  

求解机器学习问题的步骤可以分为 “学习/训练” A 和“推理”两个阶段。首先， 在学习阶段进行模型的学习 B，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系  

数据限定到某个范围内的处理称为正规化（`normalization`）。对神经网络的输入数据进行某种既定的转换称为预处理（`pre-processing`）。

预处理在神经网络（深度学习）中非常实用，其有效性已在提高识别性能和学习的效率等众多实验中得到证明。在刚才的例子中，作为一种预处理，我们将各个像素值除以 255，进行了简单的正规化。实际上，很多预处理都会考虑到数据的整体分布。比如，利用数据整体的均值或标准差，移动数据，使数据整体以 0为中心分布，或者进行正规化，把数据的延展控制在一定范围内。除此之外，还有将数据整体的分布形状均匀化的方法，即数据白化（whitening）等。  

神经网络的学习。这里所说的“学习”是指从训练数据中自动获取最优权重参数的过程。第四章中，为了使神经网络能进行学习，将导入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。为了找出尽可能小的损失函数的值，第四章我们将介绍利用了函数斜率的梯度法。  

损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的 

像这样通过将神经网络的组成元素以层的方式实现，可以轻松地构建神经网络。这个用层进行模块化的实现具有很大优点。因为想另外构建一个神经网络（比如5层、 10层、 20层……的大的神经网络）时，只需像组装乐高积木那样添加必要的层就可以了。之后，通过各个层内部实现的正向传播和反向传播，就可以正确计算进行识别处理或学习所需的梯度。   

## 二、感知机

感知机接收多个输入信号，输出一个信号 。本章中所说的感知机应该称为“人工神经元”或“朴素感知机”  

感知机中使用了阶跃函数作为激活函数,阶跃函数是指一旦输入超过阈值，就切换输出的函数。  (类似于符号函数)

输入信号被送往神经元时，会被分别乘以固定的权重。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出 `1`。这也称为 “神经元被激活” 。这里将这个界限值称为阈值，用符号 `θ` 表示 ,$b=-\theta$ ,`b` 被称为偏置 （其实就是偏移量）。

“神经元被激活” 用数学公式表达：
$$
w_1x_1+w_2x_2 > \theta \\
w_1x_1+w_2x_2 +b > 0 \\
$$

### 线性和非线性

感知机的局限性就在于它只能表示由一条直线分割的空间。弯曲的曲线无法用感知机表示。另外，由弯曲的曲线分割而成的空间称为**非线性空间**，由直线分割而成的空间称为**线性空间**。  

单层感知机可以实现与门（`AND`）、与非门（`NAND == Not AND`）、或门（`OR`）三种逻辑电路，单层感知机无法分离非线性空间 

可以通过 **多层感知机** 实现复杂逻辑电路，例如异或门

![image-20230107094501059](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/202301070945218.png)



## 三、神经网络

### sigmoid 函数

$$
h(x) = \frac{1}{1+e^{-x}}
$$

首先注意到的是“平滑性”的不同。 sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。  

感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号  

阶跃函数和sigmoid函数两者均为非线性函数，输出信号的值都在0到1之间  

### ReLU 函数

ReLU（Rectified Linear Unit）函数。  
$$
h(x) =
\left\{
\begin{aligned}
x \,\,\,\,\,&(x > 0)\\
0 \,\,\,\,\,& (x \le 0)\\
\end{aligned}
\right.
$$

ReLU 层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为ON；没有电流通过的话，就将开关设为OFF。反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，则不会有电流通过。  

### 定义符号

我们先从定义符号开始。请看图3-16。图3-16中只突出显示了从输入层神经元 $x_2$ 到后一层的神经元 $a_1^{(1)}$ 的权重。
如图3-16所示，权重和隐藏层的神经元的右上角有一个 `(1)`，它表示权重和神经元的层号（即第1层的权重、第1层的神经元）。此外，权重的右下角有两个数字，它们是后一层的神经元和前一层的神经元的索引号。比如，表示前一层的第 2 个神经元 $x_2$ 到后一层的第 1 个神经元 $a_1^{(1)}$ 的权重。

**权重右下角按照“后一层的索引号、前一层的索引号”的顺序排列。**  

![yu.3-16](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/yu.3-16.png)

### 信号传递

![yu.3-17](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/yu.3-17.png)

图3-17中增加了表示偏置的神经元 “1”。请注意，偏置的右下角的只有一个。这是因为前一层的偏置神经元（神经元“1”）只有一个。  

> 任何前一层的偏置神经元“1”都只有一个。偏置权重的数量取决于后一层的神经元的数量（不包括后一层的偏置神经元“1”）。——译者注  

$a_1^{(1)}$ 用数学表示：
$$
a_1^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1{(1)}
$$
第一层的加权和可表示为：
$$
A^{(1)} = XW^{1} +B^{(1)}
$$
其中，$A^{(1)},X,B^{(1)},W^{(1)}$ 如下图所示

![image-20230107165732917](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/202301071657966.png)



### softmax 函数

分类问题中使用的 softmax 函数可以用下式表示  
$$
y_k = \frac{e^{a_k}}{\sum_{i=1}^n{a_i}}
$$
假设输出层共有 $n$ 个神经元，计算第 $k$ 个神经元的输出 $y_k$。softmax函数的分子是输入信号 $a_k$ 的指数函数，分母是所有输入信号的指数函数的和。  

softmax 函数的输出是 $0.0$ 到 $1.0$ 之间的实数。并且，softmax 函数的输出值的总和是$1$。输出总和为 $1$ 是softmax函数的一个重要性质。正因为有了这个性质，我们才可以把softmax 函数的输出解释为 **“概率”**。  

这里需要注意的是，即便使用了softmax函数，**各个元素之间的大小关系也不会改变。**  

### 激活函数

输出层所用的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid函数，多元分类问题可以使用 softmax 函数。  

恒等函数

```python
def identity_function(x):
    return x
```

sigmoid 函数

```python
def sigmoid(x):
    return 1 / (1+np.exp(-x))
```

softmax 函数

```python
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    
    return y
```



### 三层网络的前向处理

```python
# 输入层（0层）到第一层：
    def sigmoid(x):
     	return 1 / (1+np.exp(-x))
    X = np.array([1.0,0.5])
    W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
    B1 = np.array([0.1,0.2,0.3])

    A1 = np.dot(X,W1) + B1
    Z1 = sigmoid(A1)
    print(Z1)
    print("~~~~~~~")
    # 第一层到第二层：
    W2 = np.array([[0.1,0.4] , [0.2,0.5] , [0.3,0.6]])
    B2 = np.array([0.1,0.2])

    A2 = np.dot(Z1,W2) + B2
    Z2 = sigmoid(A2)
    print(Z2)
    print("~~~~~~~")
    # 第二层到第三层：
    def identity_function(x):
        return x

    W3 = np.array([[0.1,0.3] , [0.2,0.4]])
    B3 = np.array([0.1,0.2])

    A3 = np.dot(Z2,W3) + B3
    Y = identity_function(A3)
    print(Y)
    print("~~~~~~~")
```

输出：

```bash
[0.57444252 0.66818777 0.75026011]
~~~~~~~
[0.62624937 0.7710107 ]
~~~~~~~
[0.31682708 0.69627909]
~~~~~~~
```

可以将上述代码规范化，定义 `init_network()` 和 `forward()` 函数。 `init_network()` 函数会进行权重和偏置的初始化，并将它们保存在字典变量 `network` 中。这个字典变量 `network` 中保存了每一层所需的参数（权重和偏置）。

`forward()` 函数中则封装了将输入信号转换为输出信号的处理过程。  

```python
def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])
    return network


def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)
    return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)  # [ 0.31682708 0.69627909]
```

另外，这里出现了`forward`（前向）一词，它表示的是 **从输入到输出方向**的传递处理。后面在进行神经网络的训练时，我们将介绍后向（`backward`，**从输出到输入方向**）的处理  



### 手写数字识别

这里使用的数据集是 MNIST 手写数字图像集。MNIST 是机器学习领域最有名的数据集之一，被应用于从简单的实验到发表的论文研究等各种场合。实际上，在阅读图像识别或机器学习的论文时，MNIST 数据集经常作为实验用的数据出现。

MNIST 数据集是由 0 到 9 的数字图像构成的（图 1）。训练图像有 6 万张，测试图像有 1 万张，这些图像可以用于学习和推理。MNIST 数据集的一般使用方法是，先用训练图像进行学习，再用学习到的模型度量能在多大程度上对测试图像进行正确的分类。

![img](https://static001.infoq.cn/resource/image/e3/c1/e38fe74c8dccfca9202a2202452d49c1.png)

**图 1　MNIST 图像数据集的例子**

#### 训练



MNIST 的图像数据是 28 像素 × 28 像素的灰度图像（1 通道），各个像素的取值在 0 到 255 之间。每个图像数据都相应地标有“7”“2”“1”等标签。

load_mnist函数以“(训练图像 ,训练标签 )，(测试图像，测试标签 )”的形式返回读入的MNIST数据。此外，还可以像 `load_mnist(normalize=True,
latten=True, one_hot_label=False)` 这 样，设 置 3 个 参 数。第 1 个 参 数ormalize设置是否将输入图像正规化为0.0～1.0的值。如果将该参数设置为 False，则输入图像的像素会保持原来的0～255。第2个参数 flatten设置是否展开输入图像（变成一维数组）。如果将该参数设置为 False，则输入图像为1 × 28 × 28的三维数组；若设置为True，则输入图像会保存为由784个元素构成的一维数组。第3个参数one_hot_label设置是否将标签保存为one
-hot表示（one-hot representation）。 one-hot表示是仅正确解标签为1，其余皆为0的数组，就像[0,0,1,0,0,0,0,0,0,0]这样。当one_hot_label为False时，只是像 7、 2这样简单保存正确解标签；当 one_hot_label为 True时，标签则保存为one-hot表示。  

> one-hot是零一编码，比如说你一个分类问题，有十个类，然后当前你预测来这个样本，它是第五类，那么那么那么它在第五个位置上，它的数值就是一，然后其他所有位置上的数值都是零，如果是非one-hot编的码的话，它就会输出，五就是数字五，而不是一个多维的零向量。
>
> ```bash
> 一般分类问题答案：7
> one-hot给出答案：[0, 0, 0, 0, 0, 0, 1, 0, 0]
> ```

训练代码：

```python
# coding: utf-8
import sys, os
sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定
import numpy as np
from dataset.mnist import load_mnist
from PIL import Image


def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()

(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)

img = x_train[0]
label = t_train[0]
print(label)  # 5

print(img.shape)  # (784,)
img = img.reshape(28, 28)  # 形状を元の画像サイズに変形
print(img.shape)  # (28, 28)

img_show(img)

```

#### 推理

下面，我们对这个MNIST数据集实现神经网络的推理处理。神经网络的输入层有784个神经元，输出层有10个神经元。输入层的784这个数字来源于图像大小的28 × 28 = 784，输出层的10这个数字来源于10类别分类（数字0到9，共10类别）。此外，这个神经网络有2个隐藏层，第1个隐藏层有50个神经元，第2个隐藏层有100个神经元。这个50和100可以设置为任何值。下面我们先定义 get_data()、 init_network()、 predict()这3个函数。  

首先获得MNIST数据集，生成网络。接着，用 for语句逐一取出保存在 x中的图像数据，用predict()函数进行分类。 predict()函数以NumPy数组的形式输出各个标签对应的概率。比如输出 [0.1, 0.3, 0.2, ..., 0.04] 的数组，该数组表示“0”的概率为0.1，“1” 的概率为0.3，等等。然后，我们取出这个概率列表中的最大值的索引（第几个元素的概率最高），作为预测结果。可以用 np.argmax(x) 函数取出数组中的最大值的索引， np.argmax(x) 将获取被赋给参数 x 的数组中的最大值元素的索引。最后，比较神经网络所预测的答案和正确解标签，将回答正确的概率作为识别精度。  



### 批处理 

批处理（`batch process`）对计算机的运算大有利处，可以大幅缩短每张图像的处理时间。那么为什么批处理可以缩短处理时间呢？这是因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格地讲，相对于数据读入，可以将更多的时间用在计算上）。也就是说，批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快。  



### argmax 函数

```
argmax:返回的是值最大的索引，axis=0：列，axis=1：行
```



## 四、神经网络的学习

从零开始想出一个可以识别 `5` 的算法，不如考虑通过有效利用数据来解决这个问题。一种方案是，先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括SIFT、 SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、 KNN等分类器进行学习

### 损失函数

神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为损失函数（`loss function`）。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等  

#### 均方误差

$$
E = \frac{1}{2} \sum_k (y_k-t_k)^2
$$



#### 交叉熵误差

$$
E = - \sum_k t_k \log y_k
$$



softmax函数的输出可以理解为概率 

将正确解标签表示为1，其他标签表示为0的表示方法称为 `one-hot` 表示。  

均方误差（`mean squared error`）和 交叉熵误差（`cross entropy error`） 越小越好，

交叉熵误差（`cross entropy error`）正确解标签对应的输出越大，交叉熵误差的值越接近0；当输出为1时，交叉熵误差为0。此外，如果正确解标签对应的输出较小，则交叉熵误差的值较大  

**mini-batch** 的损失函数也是利用一部分样本数据来近似地计算整体。也就是说，用随机选择的小批量数据（mini-batch）作为全体训练数据的近似值。  

### 数值微分                                                                                                                                                                                                                                                

利用微小的差分求导数的过程称为数值微分（`numerical differentiation`）。而基于数学式的推导求导数的过程，则用“解析性”（`analytic`）一词，称为“解析性求解”或者“解析性求导”。比如，$y = x^2$ 的导数 $y=2x$，可以通过解析性地求解出来。因此，当 $x = 2 $ 时，$y$ 的导数为 $4$。解析性求导得到的导数是不含误差的“真的导数”。  

```python
def numerical_diff(f,x):
    h = 1e-4 # 0.0001
    return (f(x+h) - f(x-h))/ (2*h)
```

这里我们画的是元素值为负梯度的向量 ，负梯度方向是梯度法中变量的更新方向。  

<img src="https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/202301091212774.png" style="zoom:67%;" />

梯度会指向各点处的函数值降低的方向。更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向（**局部最小值的方向**），更好的为了 loss function 而服务

机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。这里所说的最优参数是指损失函数取最小值时的参数。但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法  

用数学公式表示为：
$$
x_0 = x_0 - \eta\frac{\partial f}{\partial x_0} \\
x_1 = x_1 - \eta\frac{\partial f}{\partial x_1} \\
$$
代码表示：

```python
def gradient_descent(f,init_x,lr=0.01,step_num=100):
    x = init_x
    
    for i in range(step_num):
        grad = numerical_gradient(f,x)
        x -= lr * frad
        
    return x
```

像学习率（数学公式中的$\eta$，代码中的 `lr` (learn rate)）这样的参数称为超参数。这是一种和神经网络的参数（权重和偏置）性质不同的参数。相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。  

### 神经网络学习步骤

**前提**

神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面4个步骤。

**步骤1（ mini-batch）**

从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值。

**步骤2（计算梯度）**

为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。

**步骤3（更新参数）**

将权重参数沿梯度方向进行微小更新。  

**步骤4（重复）**

重复步骤1、步骤2、步骤3。  

神经网络的学习按照上面4个步骤进行。这个方法通过梯度下降法更新参数，不过因为这里使用的数据是随机选择的mini batch数据，所以又称为随机梯度下降法（`stochastic gradient descent`）。“随机”指的是“随机选择的”的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法”。深度学习的很多框架中，随机梯度下降法一般由一个名为SGD的函数来实现。SGD来源于随机梯度下降法的英文名称的首字母  





## 五、误差反向传播法

![yu-5.6](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/yu-5.6.png)

如图所示，反向传播的计算顺序是，将信号E乘以节点的局部导数（ $\frac{\partial y}{\partial x}$ ），然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中 $y = f(x)$ 的导数，也就是 $y$ 关于 $x$ 的导数（ $\frac{\partial y}{\partial x}$ ）。比如，假设 $y = f(x) = x^2$，则局部导数为 $\frac{\partial y}{\partial x}= 2x$。把这个局部导数乘以上游传过来的值（本例中为E），然后传递给前面的节点。  





以下式为例，阐述链式法则
$$
z = t^2 \\
t = x+y
$$
![yu.5-7](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/yu.5-7.png)

如图所示，计算图的反向传播从右到左传播信号。反向传播的计算顺序是，先将节点的输入信号乘以节点的局部导数（偏导数），然后再传递给下一个节点。比如，反向传播时，“**2”节点的输入是 $\frac{\partial z}{\partial x}$，将其乘以局部导数 $\frac{\partial z}{\partial t}$（因为正向传播时输入是t、输出是z，所以这个节点的局部导数是 $\frac{\partial z}{\partial t}$），然后传递给下一个节点。另外，图5-7中反向传播最开始的信号 $\frac{\partial z}{\partial z}$ 在前面的数学式中没有出现，这是因为 $\frac{\partial z}{\partial z} = 1$，所以在刚才的式子中被省略了。  

![yu.5-8](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/yu.5-8.png)

### 加法

不变

![image-20230112195109323](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/202301121951504.png)

因为加法节点的反向传播只是将输入信号输出到下一个节点，所以如图 5-11 所示，反向传播将 $1.3$ 向下一个节点传递。  

### 乘法

交换

![image-20230112195247259](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/202301121952344.png)

因为乘法的反向传播会乘以输入信号的翻转值，所以各自可按 $1.3 × 5 =6.5、 1.3 × 10 = 13$ 计算。另外，加法的反向传播只是将上游的值传给下游，并不需要正向传播的输入信号。但是，乘法的反向传播需要正向传播时的输入信号值。因此，实现乘法节点的反向传播时，要保存正向传播的输入信号。  

### sigmoid

前向传播：
$$
y = \frac{1}{1+e^{-x}}
$$
反向传播：
$$
y(1-y) * \frac{\partial L}{\partial y}
$$
![image-20230112200350159](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/202301122003255.png)

### Affine

前向传播：
$$
Y = XW+B
$$
反向传播：
$$
\frac{\partial L}{\partial X}=\frac{\partial L}{\partial Y} \cdot W^T \\
\frac{\partial L}{\partial W}=X^T \cdot \frac{\partial L}{\partial Y}  \\
$$
![image-20230112200458955](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/202301122004058.png)

### Softmax with Loss

#### softmax 函数

正向传播
$$
y_k = \frac{e^{a_k}}{\sum_{i=1}^n{a_i}}
$$
反向传播

![image-20230112202407693](https://gcore.jsdelivr.net/gh/L2019math/imagebed@main/202301122024761.png)



#### softmax + 交叉熵

正向传播
$$
C = -\sum_k t_k \log y_k
$$
反向传播
$$
\frac{\partial C}{\partial x_k} = y_k-t_k
$$
具体推导 [看这里](https://zhuanlan.zhihu.com/p/37740860)
